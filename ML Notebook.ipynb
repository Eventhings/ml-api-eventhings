{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2688dc30-3f19-4e18-bf1b-62ab97f6c761",
   "metadata": {},
   "source": [
    "# Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516341b2-3169-4eb8-bf93-dba0a7b4d953",
   "metadata": {},
   "source": [
    "## Collaborative-Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be07ad4c-1177-480f-9340-e19080a699d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import heapq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from random import shuffle\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Multiply, Dropout, Dense, BatchNormalization, Concatenate\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import L1, L2\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce93363-beff-4359-9b8b-745666e6362d",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "967ef78b-52c8-4ae5-b74b-07df4916e31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negatives(uids, iids, items, df_test, num_neg = 13):\n",
    "    \"\"\"\n",
    "    Returns a pandas dataframe of num_neg negative interactions\n",
    "    based for each (user, item) pair in df_test.\n",
    "    \n",
    "    Args:\n",
    "        uids (list): all user ids\n",
    "        iids (list): all item ids\n",
    "        items (list): all unique items within historical data\n",
    "        df_test (dataframe): test dataset from train_test_split\n",
    "        num_neg (int): number of negative interactions\n",
    "        \n",
    "    Returns:\n",
    "        df_neg (dataframe): dataframe with 15 negative items \n",
    "            for each (user, item) pair in df_test.\n",
    "    \"\"\"\n",
    "\n",
    "    negativeList = []\n",
    "    test_u = df_test['user_id'].values.tolist()\n",
    "    test_i = df_test['vendor_id'].values.tolist()\n",
    "\n",
    "    test_ratings = list(zip(test_u, test_i))\n",
    "    zipped = set(zip(uids, iids))\n",
    "\n",
    "    for (u, i) in test_ratings:\n",
    "        negatives = []\n",
    "        negatives.append((u, i))\n",
    "        for t in range(num_neg):\n",
    "            j = np.random.randint(len(items)) # Get random item id.\n",
    "            while (u, j) in zipped: # Check if there is an interaction\n",
    "                j = np.random.randint(len(items)) # If yes, generate a new item id\n",
    "            negatives.append(j) # Once a negative interaction is found we add it.\n",
    "        negativeList.append(negatives)\n",
    "\n",
    "    df_neg = pd.DataFrame(negativeList)\n",
    "\n",
    "    return df_neg\n",
    "\n",
    "\n",
    "def mask_first(x):\n",
    "    \"\"\"\n",
    "    Return a list of 0 for the first item and 1 for all others\n",
    "    \"\"\"\n",
    "\n",
    "    result = np.ones_like(x)\n",
    "    result[0] = 0\n",
    "    \n",
    "    return result\n",
    "   \n",
    "\n",
    "def train_test_split(df):\n",
    "    \"\"\"\n",
    "    Split dataset into training and test dataset.\n",
    "    The training set is made of all of our data except holdout items, while\n",
    "    the test set is only holdout items for each users.\n",
    "    \n",
    "    Args:\n",
    "        df (dataframe): original dataset.\n",
    "        \n",
    "    Returns:\n",
    "        df_train (dataframe): original dataset except holdout items for each users\n",
    "        df_test (dataframe): only holdout items for each users.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create two copies of our dataframe that we can modify\n",
    "    df_test = df.copy(deep = True)\n",
    "    df_train = df.copy(deep = True)\n",
    "\n",
    "    # Group by user_id and select only the first item for\n",
    "    # each user (our holdout).\n",
    "    df_test = df_test.groupby(['user_id']).first()\n",
    "    df_test['user_id'] = df_test.index\n",
    "    df_test = df_test[['user_id', 'vendor_id', 'rating']]\n",
    "\n",
    "    # Remove the same items for our test set in our training set.\n",
    "    mask = df.groupby(['user_id'])['user_id'].transform(mask_first).astype(bool)\n",
    "    df_train = df.loc[mask]\n",
    "\n",
    "    return df_train, df_test\n",
    "\n",
    "\n",
    "def load_dataset():\n",
    "    \"\"\"\n",
    "    Load data from cloud database.\n",
    "\n",
    "    Returns:\n",
    "        df_medpar (dataframe): original dataset for media partner.\n",
    "        df_rental (dataframe): original dataset for rental.\n",
    "        df_sponsor (dataframe): original dataset for sponsorship.\n",
    "    \"\"\"\n",
    "\n",
    "    df_medpar = pd.read_csv('medpar_reviews.csv')\n",
    "    df_medpar = df_medpar.rename(columns = {'mp_id': 'vendor_id'})\n",
    "    \n",
    "    df_rental = pd.read_csv('rentals_reviews.csv')\n",
    "    df_rental = df_rental.rename(columns = {'rt_id': 'vendor_id'})\n",
    "    \n",
    "    df_sponsor = pd.read_csv('sponsorship_reviews.csv')\n",
    "    df_sponsor = df_sponsor.rename(columns = {'sp_id': 'vendor_id'})\n",
    "\n",
    "    return df_medpar, df_rental, df_sponsor\n",
    "\n",
    "\n",
    "def transform_dataset(df_medpar, df_rental, df_sponsor):\n",
    "    \"\"\"\n",
    "    Transform dataset into a list of users and items as per stated here:\n",
    "    https://medium.com/@victorkohler/collaborative-filtering-using-deep-neural-networks-in-tensorflow-96e5d41a39a1\n",
    "    \n",
    "    Args:\n",
    "        df_medpar (dataframe): original dataset for media partner.\n",
    "        df_rental (dataframe): original dataset for rental.\n",
    "        df_sponsor (dataframe): original dataset for sponsorship.\n",
    "\n",
    "    Returns:\n",
    "        uids (list): all user ids.\n",
    "        iids (list): all item ids.\n",
    "        df_train (dataframe): original dataset except holdout items for each users.\n",
    "        df_test (dataframe): only holdout items for each users.\n",
    "        df_neg (dataframe): dataframe with 15 negative items\n",
    "            for each (user, item) pair in df_test.\n",
    "        users (list): all unique users.\n",
    "        items (list): all unique items.\n",
    "        label_encoder_user (LabelEncoder): encoder for user_id.\n",
    "        label_encoder_item (LabelEncoder): encoder for item_id.\n",
    "    \"\"\"\n",
    "\n",
    "    # Union the results\n",
    "    df = pd.concat([df_medpar, df_rental, df_sponsor], \n",
    "                    ignore_index = True)\n",
    "    df = df[['user_id', 'vendor_id', 'rating']]\n",
    "\n",
    "    df = df.loc[df['rating'] >= 4]\n",
    "\n",
    "    # Encode user_id and vendor_id\n",
    "    label_encoder_user = LabelEncoder()\n",
    "    label_encoder_item = LabelEncoder() \n",
    "\n",
    "    df['user_id'] = label_encoder_user.fit_transform(df['user_id'])\n",
    "    df['vendor_id'] = label_encoder_item.fit_transform(df['vendor_id'])\n",
    "\n",
    "    # Create training and test sets.\n",
    "    df_train, df_test = train_test_split(df)\n",
    "\n",
    "    # Create lists of all unique users and artists\n",
    "    users = list(df['user_id'].unique()) # Label\n",
    "    items = list(df['vendor_id'].unique()) # Label\n",
    "\n",
    "    # Get all user ids and item ids.\n",
    "    uids = df_train['user_id'].tolist() # Label\n",
    "    iids = df_train['vendor_id'].tolist() # Label\n",
    "\n",
    "    # Sample negative interactions for each user in our test data\n",
    "    df_neg = get_negatives(uids, iids, items, df_test) # Label\n",
    "    \n",
    "    return uids, iids, df_train, df_test, df_neg, users, items, label_encoder_user, label_encoder_item\n",
    "\n",
    "\n",
    "def get_train_instances(uids, iids, items, portion = 0.05):\n",
    "    \"\"\"\n",
    "    Samples a number of negative user-item interactions for each\n",
    "    user-item pair in our testing data.\n",
    "\n",
    "    Args:\n",
    "        uids (list): all users' id within the historical data.\n",
    "        iids (list): all items' id within the historical data.\n",
    "        items (list): list of all unique items.\n",
    "        portion (float): portion of items used for training.\n",
    "     \n",
    "    Returns:\n",
    "        user_input (list): A list of all users for each item.\n",
    "        item_input (list): A list of all items for every user,\n",
    "            both positive and negative interactions.\n",
    "        labels (list): A list of all labels. 0 or 1.\n",
    "    \"\"\"\n",
    "\n",
    "    # Number of negative interactions for fitting\n",
    "    num_neg = int(portion * len(items) // 50) * 50\n",
    "\n",
    "    # Training instances\n",
    "    user_input, item_input, labels = [], [], []\n",
    "    zipped = set(zip(uids, iids))\n",
    "    \n",
    "    for (u, i) in zip(uids, iids):\n",
    "        # Add our positive interaction\n",
    "        user_input.append(u)\n",
    "        item_input.append(i)\n",
    "        labels.append(1)\n",
    "\n",
    "        # Sample a number of random negative interactions\n",
    "        for t in range(num_neg):\n",
    "            j = np.random.randint(len(items))\n",
    "            while (u, j) in zipped:\n",
    "                j = np.random.randint(len(items))\n",
    "            user_input.append(u)\n",
    "            item_input.append(j)\n",
    "            labels.append(0)\n",
    "\n",
    "    return user_input, item_input, labels\n",
    "\n",
    "\n",
    "def get_hits(k_ranked, holdout):\n",
    "    \"\"\"\n",
    "    Return 1 if the holdout item exists in a given list and 0 if not.\n",
    "    \"\"\"\n",
    "\n",
    "    for item in k_ranked:\n",
    "        if item == holdout:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def get_ndcg(ranklist, gtItem):\n",
    "    \"\"\"\n",
    "    NDCG@k is a ranking metric that helps consider \n",
    "    both the relevance of items and their positions in the list.\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def predict_ratings_cf(user_id, items, model, label_user, label_item):\n",
    "    \"\"\"\n",
    "    Predict rating score for each user across all items.\n",
    "\n",
    "    Args:\n",
    "        user_id (int): current user_id.\n",
    "        items (list): all unique items within historical data.\n",
    "        model (h5): recsys tensorflow model.\n",
    "        label_user (int): label encoder for user.\n",
    "        label_item (int): label encoder for item.\n",
    "        \n",
    "    Returns:\n",
    "        map_item_score (list): predicted current user rating for each item.\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare user and item arrays for the model.\n",
    "    user_id = label_user.transform([user_id])[0] # Label\n",
    "    predict_user = np.full(len(items), user_id, dtype = 'int32').reshape(-1, 1) # Label\n",
    "    np_items = np.array(items).reshape(-1, 1)\n",
    "\n",
    "    # Predict ratings using the model.\n",
    "    predictions = model.predict([predict_user, np_items]).flatten().tolist()\n",
    "\n",
    "    # Map predicted score to item id.\n",
    "    items = label_item.inverse_transform(items) # Original\n",
    "    map_item_score = dict(zip(items, predictions))\n",
    "\n",
    "    return map_item_score\n",
    "    \n",
    "\n",
    "def eval_rating(idx, test_ratings, test_negatives, K, model, label_user, label_item):\n",
    "    \"\"\"\n",
    "    Generate ratings for the users in our test set and\n",
    "    check if our holdout item is among the top K item with the highest scores\n",
    "    and evaluate its position.\n",
    "    \n",
    "    Args:\n",
    "        idx (int): current user_id in label encoding.\n",
    "        test_ratings (list): test dataset (user, item) pairs.\n",
    "        test_negatives (list): negative items for each user in test dataset.\n",
    "        K (int): number of top recommendations.\n",
    "        label_user (int): label encoder for user.\n",
    "        label_item (int): label encoder for item.\n",
    "        \n",
    "    Returns:\n",
    "        hitrate (list): A list of 1 if the holdout appeared in our\n",
    "            top K predicted items. 0 if not.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the negative interactions for our user.\n",
    "    items = test_negatives[idx] # Label\n",
    "\n",
    "    # Get the user idx.\n",
    "    user_idx = test_ratings[idx][0] # Label\n",
    "    user_idx = label_user.inverse_transform([user_idx])[0] # Original\n",
    "\n",
    "    # Get the item idx, i.e., our holdout item.\n",
    "    holdout = test_ratings[idx][1] # Label\n",
    "\n",
    "    # Add the holdout to the end of the negative interactions list.\n",
    "    items.append(holdout) # Label\n",
    "\n",
    "    # Predict ratings using the model.\n",
    "    map_item_score = predict_ratings_cf(user_idx, items, model, label_user, label_item) # Original\n",
    "    items.pop()\n",
    "\n",
    "    # Get the K highest ranked items as a list.\n",
    "    k_ranked = heapq.nlargest(K, map_item_score, key = map_item_score.get) # Original\n",
    "    k_ranked = label_item.transform(k_ranked) # Label\n",
    "\n",
    "    # Get a list of hit or no hit.\n",
    "    hitrate = get_hits(k_ranked, holdout)\n",
    "    ndcg = get_ndcg(k_ranked, holdout)\n",
    "\n",
    "    return (hitrate, ndcg)\n",
    "\n",
    "\n",
    "def evaluate(model, df_test, df_neg, label_user, label_item, K = 10):\n",
    "    \"\"\"\n",
    "    Calculate the top@K hit ratio for our recommendations.\n",
    "    \n",
    "    Args:\n",
    "        df_neg (dataframe): dataframe containing our holdout items\n",
    "            and K randomly sampled negative interactions for each\n",
    "            (user, item) holdout pair.\n",
    "        label_user (int): label encoder for user.\n",
    "        label_item (int): label encoder for item.\n",
    "        K (int): number of recommended items.\n",
    "            \n",
    "    Returns:\n",
    "        hits (list): list of \"hits\". 1 if the holdout was present in \n",
    "            the K highest ranked predictions. 0 if not. \n",
    "    \"\"\"\n",
    "\n",
    "    hitrates = []\n",
    "    ndcgs = []\n",
    "\n",
    "    test_u = df_test['user_id'].values.tolist() # Label\n",
    "    test_i = df_test['vendor_id'].values.tolist() # Label\n",
    "\n",
    "    test_ratings = list(zip(test_u, test_i))\n",
    "\n",
    "    df_neg = df_neg.drop(df_neg.columns[0], axis = 1)\n",
    "    test_negatives = df_neg.values.tolist()\n",
    "\n",
    "    for user_id in test_u: # Label\n",
    "        # For each idx, call eval_one_rating\n",
    "        (hitrate, ndcg) = eval_rating(user_id, test_ratings, test_negatives, K, model, label_user, label_item)\n",
    "        hitrates.append(hitrate)\n",
    "        ndcgs.append(ndcg)\n",
    "\n",
    "    return (hitrates, ndcgs)\n",
    "\n",
    "\n",
    "def get_top_k_items(dct, k = 10):\n",
    "    \"\"\"\n",
    "    Get the top k items for the recommendations\n",
    "    \n",
    "    Args:\n",
    "        dct (dict): dictionary of \"hits\" for each item\n",
    "        k (int): number of recommended items\n",
    "            \n",
    "    Returns:\n",
    "        average_ratings (list): list of average ratings for each item\n",
    "        recommendation_items (list): list of recommended items\n",
    "    \"\"\"\n",
    "\n",
    "    # Use nlargest to get the top n key-value pairs based on values.\n",
    "    top_k_items = heapq.nlargest(k, dct.items(), key = lambda item: item[1])\n",
    "\n",
    "    # Calculate avg rating for each item.\n",
    "    average_ratings = [item[1] for item in top_k_items]\n",
    "    recommendation_items = [item[0] for item in top_k_items]\n",
    "\n",
    "    # Convert all items to strings.\n",
    "    average_ratings = [str(item) for item in average_ratings]\n",
    "    recommendation_items = [str(item) for item in recommendation_items]\n",
    "\n",
    "    return average_ratings, recommendation_items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aec3ba-3f0e-4103-a20c-e28f25835662",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3273aac-dfaa-4c44-bf74-3af3157fd7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df_medpar, df_rental, df_sponsor = load_dataset()\n",
    "uids, iids, df_train, df_test, df_neg, users, items, label_encoder_user, label_encoder_item = transform_dataset(df_medpar, df_rental, df_sponsor)\n",
    "\n",
    "# HYPERPARAMS\n",
    "epochs = 10\n",
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e14a1d3-a88f-45f3-bbba-cef94c59c06c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\c640\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\c640\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " user (InputLayer)           [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " item (InputLayer)           [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " mlp_user_embedding (Embedd  (None, 1, 20)                400       ['user[0][0]']                \n",
      " ing)                                                                                             \n",
      "                                                                                                  \n",
      " mlp_item_embedding (Embedd  (None, 1, 20)                44140     ['item[0][0]']                \n",
      " ing)                                                                                             \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)         (None, 20)                   0         ['mlp_user_embedding[0][0]']  \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)         (None, 20)                   0         ['mlp_item_embedding[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 40)                   0         ['flatten_2[0][0]',           \n",
      "                                                                     'flatten_3[0][0]']           \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 40)                   0         ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " mlp_layer1 (Dense)          (None, 64)                   2624      ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " mlp_batch_norm1 (BatchNorm  (None, 64)                   256       ['mlp_layer1[0][0]']          \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " mlp_dropout1 (Dropout)      (None, 64)                   0         ['mlp_batch_norm1[0][0]']     \n",
      "                                                                                                  \n",
      " mlp_layer2 (Dense)          (None, 32)                   2080      ['mlp_dropout1[0][0]']        \n",
      "                                                                                                  \n",
      " mlp_batch_norm2 (BatchNorm  (None, 32)                   128       ['mlp_layer2[0][0]']          \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " gmf_user_embedding (Embedd  (None, 1, 20)                400       ['user[0][0]']                \n",
      " ing)                                                                                             \n",
      "                                                                                                  \n",
      " gmf_item_embedding (Embedd  (None, 1, 20)                44140     ['item[0][0]']                \n",
      " ing)                                                                                             \n",
      "                                                                                                  \n",
      " mlp_dropout2 (Dropout)      (None, 32)                   0         ['mlp_batch_norm2[0][0]']     \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 20)                   0         ['gmf_user_embedding[0][0]']  \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)         (None, 20)                   0         ['gmf_item_embedding[0][0]']  \n",
      "                                                                                                  \n",
      " mlp_layer3 (Dense)          (None, 16)                   528       ['mlp_dropout2[0][0]']        \n",
      "                                                                                                  \n",
      " multiply (Multiply)         (None, 20)                   0         ['flatten[0][0]',             \n",
      "                                                                     'flatten_1[0][0]']           \n",
      "                                                                                                  \n",
      " mlp_layer4 (Dense)          (None, 8)                    136       ['mlp_layer3[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate  (None, 28)                   0         ['multiply[0][0]',            \n",
      " )                                                                   'mlp_layer4[0][0]']          \n",
      "                                                                                                  \n",
      " output_layer (Dense)        (None, 1)                    29        ['concatenate_1[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 94861 (370.55 KB)\n",
      "Trainable params: 94669 (369.80 KB)\n",
      "Non-trainable params: 192 (768.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# TENSORFLOW GRAPH\n",
    "# Using the functional API\n",
    "\n",
    "# HYPERPARAMS\n",
    "latent_features = 20\n",
    "learning_rate = 0.005\n",
    "\n",
    "# TENSORFLOW GRAPH\n",
    "# Using the functional API\n",
    "\n",
    "# Define input layers for user, item, and label.\n",
    "user_input = Input(shape = (1,), dtype = tf.int32, name = 'user')\n",
    "item_input = Input(shape = (1,), dtype = tf.int32, name = 'item')\n",
    "label_input = Input(shape = (1,), dtype = tf.int32, name = 'label')\n",
    "\n",
    "# User embedding for MLP\n",
    "mlp_user_embedding = Embedding(input_dim = len(users), \n",
    "                                output_dim = latent_features,\n",
    "                                embeddings_initializer = 'random_normal',\n",
    "                                embeddings_regularizer = L1(0.01),\n",
    "                                input_length = 1, \n",
    "                                name = 'mlp_user_embedding')(user_input)\n",
    "\n",
    "# Item embedding for MLP\n",
    "mlp_item_embedding = Embedding(input_dim = len(items), \n",
    "                               output_dim = latent_features,\n",
    "                               embeddings_initializer = 'random_normal',\n",
    "                               embeddings_regularizer = L1(0.01),\n",
    "                               input_length = 1, \n",
    "                               name = 'mlp_item_embedding')(item_input)\n",
    "\n",
    "# User embedding for GMF\n",
    "gmf_user_embedding = Embedding(input_dim = len(users), \n",
    "                               output_dim = latent_features,\n",
    "                               embeddings_initializer = 'random_normal',\n",
    "                               embeddings_regularizer = L1(0.01),\n",
    "                               input_length = 1, \n",
    "                               name = 'gmf_user_embedding')(user_input)\n",
    "\n",
    "# Item embedding for GMF\n",
    "gmf_item_embedding = Embedding(input_dim = len(items), \n",
    "                               output_dim = latent_features,\n",
    "                               embeddings_initializer = 'random_normal',\n",
    "                               embeddings_regularizer = L1(0.01),\n",
    "                               input_length = 1, \n",
    "                               name = 'gmf_item_embedding')(item_input)\n",
    "\n",
    "# GMF layers\n",
    "gmf_user_flat = Flatten()(gmf_user_embedding)\n",
    "gmf_item_flat = Flatten()(gmf_item_embedding)\n",
    "gmf_matrix = Multiply()([gmf_user_flat, gmf_item_flat])\n",
    "\n",
    "# MLP layers\n",
    "mlp_user_flat = Flatten()(mlp_user_embedding)\n",
    "mlp_item_flat = Flatten()(mlp_item_embedding)\n",
    "mlp_concat = Concatenate()([mlp_user_flat, mlp_item_flat])\n",
    "\n",
    "mlp_dropout = Dropout(0.1)(mlp_concat)\n",
    "\n",
    "mlp_layer_1 = Dense(64, \n",
    "                    activation = 'relu', \n",
    "                    name = 'mlp_layer1')(mlp_dropout)\n",
    "mlp_batch_norm1 = BatchNormalization(name = 'mlp_batch_norm1')(mlp_layer_1)\n",
    "mlp_dropout1 = Dropout(0.1, \n",
    "                    name = 'mlp_dropout1')(mlp_batch_norm1)\n",
    "\n",
    "mlp_layer_2 = Dense(32, \n",
    "                    activation = 'relu', \n",
    "                    name = 'mlp_layer2')(mlp_dropout1)\n",
    "mlp_batch_norm2 = BatchNormalization(name = 'mlp_batch_norm2')(mlp_layer_2)\n",
    "mlp_dropout2 = Dropout(0.1, \n",
    "                    name = 'mlp_dropout2')(mlp_batch_norm2)\n",
    "\n",
    "mlp_layer_3 = Dense(16, \n",
    "                    activation = 'relu', \n",
    "                    kernel_regularizer = L2(0.01),\n",
    "                    name = 'mlp_layer3')(mlp_dropout2)\n",
    "mlp_layer_4 = Dense(8, \n",
    "                    activation = 'relu', \n",
    "                    activity_regularizer = L2(0.01),\n",
    "                    name = 'mlp_layer4')(mlp_layer_3)\n",
    "\n",
    "# Merge the two networks\n",
    "merged_vector = Concatenate()([gmf_matrix, mlp_layer_4])\n",
    "\n",
    "# Output layer\n",
    "output_layer = Dense(1, \n",
    "                    activation = 'sigmoid',\n",
    "                    kernel_initializer = 'lecun_uniform',\n",
    "                    name = 'output_layer')(merged_vector)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs = [user_input, item_input], outputs = output_layer)\n",
    "\n",
    "# Compile the model with binary cross entropy loss and Adam optimizer\n",
    "optimizer = Adam(learning_rate = learning_rate)\n",
    "model.compile(optimizer = optimizer,\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eeb6ece0-33e3-46e1-bd78-467c6801231e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 313ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "0.8\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "0.7\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 227ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "0.8\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "0.7\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "0.7\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "0.75\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 140ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "0.6\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "0.75\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "0.6\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "0.65\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    # Get our training input.\n",
    "    user_input, item_input, labels = get_train_instances(uids, iids, items)\n",
    "\n",
    "    # Training        \n",
    "    hist = model.fit([np.array(user_input), np.array(item_input)], #input\n",
    "                     np.array(labels), # labels \n",
    "                     batch_size = batch_size, \n",
    "                     verbose = 0, \n",
    "                     shuffle = True)\n",
    "\n",
    "    # Evaluation\n",
    "    (hitrates, ndcgs) = evaluate(model, df_test, df_neg, label_encoder_user, label_encoder_item)\n",
    "    hitrates_avg, ndcgs_avg, loss = np.array(hitrates).mean(), np.array(ndcgs).mean(), hist.history['loss'][0]\n",
    "    print(hitrates_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c069c4-5d92-4a5a-ac7f-52a31527d385",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1ab3d143-4341-40c3-96f2-060627b85160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\c640\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('collaborative-filtering.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f557ec23-2808-46fd-9e70-60beb3704aca",
   "metadata": {},
   "source": [
    "## Content-Based Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51434826-6f22-4126-bdce-adf9d29cf44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\c640\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e256d9-7bda-4717-a72a-a9853d1a7063",
   "metadata": {},
   "source": [
    "### Import Data (Content = Business's Description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0715e20-7842-4aa6-a705-97ee1d38ea94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>category</th>\n",
       "      <th>sub-category</th>\n",
       "      <th>about</th>\n",
       "      <th>metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Asus Indonesia</td>\n",
       "      <td>Sponsor</td>\n",
       "      <td>Technology Hardware</td>\n",
       "      <td>ASUS is passionate about technology and driven...</td>\n",
       "      <td>Sponsor Technology Hardware ASUS is passionate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hydro Coco</td>\n",
       "      <td>Sponsor</td>\n",
       "      <td>Food &amp; Beverage</td>\n",
       "      <td>Tentang Hydro Coco\\nHydro Coco terbuat dari ai...</td>\n",
       "      <td>Sponsor Food &amp; Beverage Tentang Hydro Coco\\nHy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nutrijell</td>\n",
       "      <td>Sponsor</td>\n",
       "      <td>Food &amp; Beverage</td>\n",
       "      <td>Nutrijell is a leading agar-agar brand in Indo...</td>\n",
       "      <td>Sponsor Food &amp; Beverage Nutrijell is a leading...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Zalora Indonesia</td>\n",
       "      <td>Sponsor</td>\n",
       "      <td>E-Commerce</td>\n",
       "      <td>Zalora Indonesia, the largest online fashion r...</td>\n",
       "      <td>Sponsor E-Commerce Zalora Indonesia, the large...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Marina</td>\n",
       "      <td>Sponsor</td>\n",
       "      <td>Personal &amp; Beauty</td>\n",
       "      <td>Marina Natural is a leading Indonesian cosmeti...</td>\n",
       "      <td>Sponsor Personal &amp; Beauty Marina Natural is a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>photobooth wedding murah jabodetabek</td>\n",
       "      <td>Equipment Rental</td>\n",
       "      <td>Photobooth</td>\n",
       "      <td>photobooth wedding murah jabodetabek</td>\n",
       "      <td>Equipment Rental Photobooth photobooth wedding...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>photo booth ulang tahun murah</td>\n",
       "      <td>Equipment Rental</td>\n",
       "      <td>Photobooth</td>\n",
       "      <td>photo booth ulang tahun murah</td>\n",
       "      <td>Equipment Rental Photobooth photo booth ulang ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>MobileTrans - Wa Transfer dan Line Transfer Or...</td>\n",
       "      <td>Equipment Rental</td>\n",
       "      <td>Photobooth</td>\n",
       "      <td>MobileTrans - Wa Transfer dan Line Transfer Or...</td>\n",
       "      <td>Equipment Rental Photobooth MobileTrans - Wa T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>SEWA WIFI JEPANG | JAPAN WIFI | SEWA JAPAN WIFI</td>\n",
       "      <td>Equipment Rental</td>\n",
       "      <td>Photobooth</td>\n",
       "      <td>SEWA WIFI JEPANG | JAPAN WIFI | SEWA JAPAN WIFI</td>\n",
       "      <td>Equipment Rental Photobooth SEWA WIFI JEPANG |...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>drfone Whatsapp Transfer Original [KHUSUS MAC]...</td>\n",
       "      <td>Equipment Rental</td>\n",
       "      <td>Photobooth</td>\n",
       "      <td>drfone Whatsapp Transfer Original [KHUSUS MAC]...</td>\n",
       "      <td>Equipment Rental Photobooth drfone Whatsapp Tr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>862 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  name          category  \\\n",
       "0                                       Asus Indonesia           Sponsor   \n",
       "1                                           Hydro Coco           Sponsor   \n",
       "2                                            Nutrijell           Sponsor   \n",
       "3                                     Zalora Indonesia           Sponsor   \n",
       "4                                               Marina           Sponsor   \n",
       "..                                                 ...               ...   \n",
       "857               photobooth wedding murah jabodetabek  Equipment Rental   \n",
       "858                      photo booth ulang tahun murah  Equipment Rental   \n",
       "859  MobileTrans - Wa Transfer dan Line Transfer Or...  Equipment Rental   \n",
       "860    SEWA WIFI JEPANG | JAPAN WIFI | SEWA JAPAN WIFI  Equipment Rental   \n",
       "861  drfone Whatsapp Transfer Original [KHUSUS MAC]...  Equipment Rental   \n",
       "\n",
       "            sub-category                                              about  \\\n",
       "0    Technology Hardware  ASUS is passionate about technology and driven...   \n",
       "1        Food & Beverage  Tentang Hydro Coco\\nHydro Coco terbuat dari ai...   \n",
       "2        Food & Beverage  Nutrijell is a leading agar-agar brand in Indo...   \n",
       "3             E-Commerce  Zalora Indonesia, the largest online fashion r...   \n",
       "4      Personal & Beauty  Marina Natural is a leading Indonesian cosmeti...   \n",
       "..                   ...                                                ...   \n",
       "857           Photobooth               photobooth wedding murah jabodetabek   \n",
       "858           Photobooth                      photo booth ulang tahun murah   \n",
       "859           Photobooth  MobileTrans - Wa Transfer dan Line Transfer Or...   \n",
       "860           Photobooth    SEWA WIFI JEPANG | JAPAN WIFI | SEWA JAPAN WIFI   \n",
       "861           Photobooth  drfone Whatsapp Transfer Original [KHUSUS MAC]...   \n",
       "\n",
       "                                              metadata  \n",
       "0    Sponsor Technology Hardware ASUS is passionate...  \n",
       "1    Sponsor Food & Beverage Tentang Hydro Coco\\nHy...  \n",
       "2    Sponsor Food & Beverage Nutrijell is a leading...  \n",
       "3    Sponsor E-Commerce Zalora Indonesia, the large...  \n",
       "4    Sponsor Personal & Beauty Marina Natural is a ...  \n",
       "..                                                 ...  \n",
       "857  Equipment Rental Photobooth photobooth wedding...  \n",
       "858  Equipment Rental Photobooth photo booth ulang ...  \n",
       "859  Equipment Rental Photobooth MobileTrans - Wa T...  \n",
       "860  Equipment Rental Photobooth SEWA WIFI JEPANG |...  \n",
       "861  Equipment Rental Photobooth drfone Whatsapp Tr...  \n",
       "\n",
       "[862 rows x 5 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"sponsors.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e93764-f78d-4356-8abf-6714384e3a79",
   "metadata": {},
   "source": [
    "### Encode All About to a Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dcce8dfc-2a98-4851-b649-3fbe4fd38391",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\c640\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "bow = CountVectorizer(stop_words=\"english\", min_df=1, tokenizer=word_tokenize)\n",
    "bank = bow.fit_transform(df.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7208a6c-04dd-432b-bb48-e14f60125424",
   "metadata": {},
   "source": [
    "### Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4c985f-1b92-4a12-baa5-a36a63bc1b0e",
   "metadata": {},
   "source": [
    "#### 1: Encode What User Click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aedf0a0a-85c9-4c48-a31b-c5d9aaaec45e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sponsor Personal & Beauty Makeover is an Indonesian cosmetics brand offering a comprehensive range of makeup and skincare products. With a focus on providing high-quality and on-trend beauty items, Makeover has gained popularity among consumers seeking both traditional and modern beauty solutions.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 66\n",
    "\n",
    "content = df.loc[idx, \"metadata\"]\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f42e44e8-55cc-489f-9128-0bd294ea79a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code = bow.transform([content])\n",
    "code.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfb93ba-0506-48ab-939e-851d700827ba",
   "metadata": {},
   "source": [
    "#### 2: Document Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82752fbe-974e-48aa-a8f6-ff6799b67998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.75851277, 0.88859377, 0.70621517, 0.78445245, 0.68715064,\n",
       "        0.69543382, 0.74952253, 0.79675608, 0.70670577, 0.73547997,\n",
       "        0.70997345, 0.78054973, 0.7451983 , 0.71042975, 0.73489413,\n",
       "        0.75828058, 0.76216464, 0.78841818, 0.76566679, 0.72397378,\n",
       "        0.75241797, 0.74215602, 0.68813284, 0.76901369, 0.77016514,\n",
       "        0.70610923, 0.7498918 , 0.57269639, 0.72998212, 0.8059039 ,\n",
       "        0.83697217, 0.83295903, 0.77571935, 0.75483361, 0.77980999,\n",
       "        0.67600313, 0.81229654, 0.75060813, 0.7628821 , 0.7475866 ,\n",
       "        0.6448576 , 0.67463742, 0.66238815, 0.79566466, 0.73660671,\n",
       "        0.73917973, 0.79826335, 0.78510261, 0.79903003, 0.73752467,\n",
       "        0.81442313, 0.75914517, 0.73057466, 0.7606225 , 0.77169417,\n",
       "        0.71067834, 0.42616442, 0.79380348, 0.7634779 , 0.76053963,\n",
       "        0.42758022, 0.73959275, 0.63968168, 0.7432237 , 0.79193741,\n",
       "        0.74316525, 0.        , 0.83354992, 0.80097892, 0.76671526,\n",
       "        0.76194412, 0.65060117, 0.76671526, 0.61196126, 0.78123451,\n",
       "        0.72835616, 0.75492997, 0.73305598, 0.78514513, 0.72354147,\n",
       "        0.51789813, 0.74785068, 0.79382017, 0.55173455, 0.71565616,\n",
       "        0.72111332, 0.73608384, 0.76537067, 0.951205  , 0.74472974,\n",
       "        0.75962491, 0.79123664, 0.72110045, 0.76436057, 0.7396131 ,\n",
       "        0.77541637, 0.72509356, 0.77766011, 0.75331955, 0.7667729 ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.88427249,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.97463269, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.96015905, 1.        , 1.        ,\n",
       "        1.        , 0.96015905, 0.96015905, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.96015905,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.9272607 , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.87895449, 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.87628209, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.96142416, 1.        ,\n",
       "        1.        , 0.95347579, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.951205  , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.91752139, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.90240999, 1.        , 0.94856555, 1.        , 1.        ,\n",
       "        1.        , 0.87484346, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.9587607 , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.9144079 , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.89089105, 1.        ,\n",
       "        1.        , 0.9587607 , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.92031809, 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.9326565 , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.89089105, 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.94856555, 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.96142416, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.95347579, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.89089105, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.9144079 , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.87484346, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.96015905,\n",
       "        1.        , 0.96015905, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.92031809,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.95545646,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.96015905,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.9587607 , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.96015905, 0.96015905, 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.96015905, 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.96142416, 1.        , 0.95720395,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.96015905, 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.9587607 , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist = cosine_distances(code, bank)\n",
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ce4fa18-e9e4-448b-a01c-e02391722f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([56, 60, 80, 83, 27, 73, 62, 40, 71, 42], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_idx = dist.argsort()[0, 1:11]\n",
    "rec_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895b2bbb-b7c8-4e85-bd3f-f5bcb93bfd53",
   "metadata": {},
   "source": [
    "#### 3: Recommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f3a4b8-1bd5-47af-8404-8abcbe02421e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[rec_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a5f1e9-6e94-441b-8431-76fd6bf62675",
   "metadata": {},
   "source": [
    "### MLE: Sum Them All Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5ccf638f-4cd1-466e-b74b-07ae8a0ff2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecommenderSystem:\n",
    "    def __init__(self, data, content_col):\n",
    "        self.df = pd.read_csv(data)\n",
    "        self.content_col = content_col\n",
    "        self.encoder = None\n",
    "        self.model = None\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            self.df['metadata'], self.df['category'], test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "        label_encoder = LabelEncoder()\n",
    "        y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "        y_val_encoded = label_encoder.transform(y_val)\n",
    "\n",
    "        return X_train, X_val, y_train_encoded, y_val_encoded\n",
    "\n",
    "    def train_model(self, X_train, y_train, X_val, y_val, epochs=10):\n",
    "        self.encoder = CountVectorizer(stop_words=\"english\", tokenizer=word_tokenize)\n",
    "        X_train_encoded = self.encoder.fit_transform(X_train).toarray()\n",
    "        X_val_encoded = self.encoder.transform(X_val).toarray()\n",
    "\n",
    "        model = Sequential([\n",
    "            Dense(128, activation='relu', input_shape=(X_train_encoded.shape[1],)),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(len(np.unique(y_train)), activation='softmax')\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        history = model.fit(X_train_encoded, y_train, epochs=epochs, validation_data=(X_val_encoded, y_val), verbose=1)\n",
    "\n",
    "        # Extract final training and validation accuracy\n",
    "        final_train_accuracy = history.history['accuracy'][-1]\n",
    "        final_val_accuracy = history.history['val_accuracy'][-1]\n",
    "\n",
    "        print(f\"Final Training Accuracy: {final_train_accuracy * 100:.2f}%\")\n",
    "        print(f\"Final Validation Accuracy: {final_val_accuracy * 100:.2f}%\")\n",
    "\n",
    "        # Save the trained model\n",
    "        model.save(\"content-based-filtering.h5\")\n",
    "\n",
    "        return model, history\n",
    "\n",
    "    def plot_accuracy(self, history):\n",
    "        # Plot training and validation accuracy\n",
    "        plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def fit(self):\n",
    "        self.encoder = CountVectorizer(stop_words=\"english\", tokenizer=word_tokenize)\n",
    "        self.bank = self.encoder.fit_transform(self.df[self.content_col])\n",
    "\n",
    "    def recommend(self, idx, topk=10):\n",
    "        content = df.loc[idx, self.content_col]\n",
    "        code = self.encoder.transform([content])\n",
    "        dist = cosine_distances(code, self.bank)\n",
    "        rec_idx = dist.argsort()[0, 1:(topk+1)]\n",
    "        return self.df.loc[rec_idx]\n",
    "\n",
    "    def run(self):\n",
    "        X_train, X_val, y_train, y_val = self.preprocess_data()\n",
    "        self.model, history = self.train_model(X_train, y_train, X_val, y_val, epochs=10)\n",
    "        self.plot_accuracy(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "21f084a8-53e1-4712-9e0e-5dab5e8d1c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\c640\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "22/22 [==============================] - 1s 21ms/step - loss: 0.3270 - accuracy: 0.9637 - val_loss: 0.0561 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.0168 - accuracy: 0.9985 - val_loss: 0.0044 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 8.7752e-04 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 6.4889e-04 - accuracy: 1.0000 - val_loss: 9.7522e-04 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 5.3736e-04 - accuracy: 1.0000 - val_loss: 7.7193e-04 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 3.6941e-04 - accuracy: 1.0000 - val_loss: 6.6266e-04 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 3.0557e-04 - accuracy: 1.0000 - val_loss: 5.7105e-04 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 2.4754e-04 - accuracy: 1.0000 - val_loss: 5.0875e-04 - val_accuracy: 1.0000\n",
      "Final Training Accuracy: 100.00%\n",
      "Final Validation Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\c640\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "recsys = RecommenderSystem(\"sponsors.csv\", content_col=\"metadata\")\n",
    "recsys.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "665ba620-d7ac-4aee-bba0-e5bbb78ffc33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>category</th>\n",
       "      <th>sub-category</th>\n",
       "      <th>about</th>\n",
       "      <th>metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Mustika Ratu</td>\n",
       "      <td>Sponsor</td>\n",
       "      <td>Personal &amp; Beauty</td>\n",
       "      <td>Mustika Ratu is a well-known Indonesian cosmet...</td>\n",
       "      <td>Sponsor Personal &amp; Beauty Mustika Ratu is a we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Holika-holika</td>\n",
       "      <td>Sponsor</td>\n",
       "      <td>Personal &amp; Beauty</td>\n",
       "      <td>Holika-holika is a South Korean cosmetics bran...</td>\n",
       "      <td>Sponsor Personal &amp; Beauty Holika-holika is a S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Wardah</td>\n",
       "      <td>Sponsor</td>\n",
       "      <td>Personal &amp; Beauty</td>\n",
       "      <td>Wardah is an Indonesian cosmetics and skincare...</td>\n",
       "      <td>Sponsor Personal &amp; Beauty Wardah is an Indones...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>The Body Shop</td>\n",
       "      <td>Sponsor</td>\n",
       "      <td>Personal &amp; Beauty</td>\n",
       "      <td>The Body Shop is a global beauty and cosmetics...</td>\n",
       "      <td>Sponsor Personal &amp; Beauty The Body Shop is a g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Paragon</td>\n",
       "      <td>Sponsor</td>\n",
       "      <td>Personal &amp; Beauty</td>\n",
       "      <td>Paragon Technology and Innovation is an Indone...</td>\n",
       "      <td>Sponsor Personal &amp; Beauty Paragon Technology a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Nivea</td>\n",
       "      <td>Sponsor</td>\n",
       "      <td>Personal &amp; Beauty</td>\n",
       "      <td>Nivea is a global brand that primarily focuses...</td>\n",
       "      <td>Sponsor Personal &amp; Beauty Nivea is a global br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Damn I Love Indonesia</td>\n",
       "      <td>Sponsor</td>\n",
       "      <td>Textiles &amp; Apparel</td>\n",
       "      <td>Damn I Love Indonesia (DILo) is a lifestyle br...</td>\n",
       "      <td>Sponsor Textiles &amp; Apparel Damn I Love Indones...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Head &amp; Shoulders</td>\n",
       "      <td>Sponsor</td>\n",
       "      <td>Personal &amp; Beauty</td>\n",
       "      <td>Head &amp; Shoulders is a well-known brand of anti...</td>\n",
       "      <td>Sponsor Personal &amp; Beauty Head &amp; Shoulders is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Bukalapak</td>\n",
       "      <td>Sponsor</td>\n",
       "      <td>E-Commerce</td>\n",
       "      <td>Bukalapak is an Indonesian online marketplace ...</td>\n",
       "      <td>Sponsor E-Commerce Bukalapak is an Indonesian ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Alfamart</td>\n",
       "      <td>Sponsor</td>\n",
       "      <td>Distribution &amp; Retail</td>\n",
       "      <td>Alfamart is a leading Indonesian convenience s...</td>\n",
       "      <td>Sponsor Distribution &amp; Retail Alfamart is a le...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     name category           sub-category  \\\n",
       "56           Mustika Ratu  Sponsor      Personal & Beauty   \n",
       "60          Holika-holika  Sponsor      Personal & Beauty   \n",
       "80                 Wardah  Sponsor      Personal & Beauty   \n",
       "83          The Body Shop  Sponsor      Personal & Beauty   \n",
       "27                Paragon  Sponsor      Personal & Beauty   \n",
       "73                  Nivea  Sponsor      Personal & Beauty   \n",
       "62  Damn I Love Indonesia  Sponsor     Textiles & Apparel   \n",
       "40       Head & Shoulders  Sponsor      Personal & Beauty   \n",
       "71              Bukalapak  Sponsor             E-Commerce   \n",
       "42               Alfamart  Sponsor  Distribution & Retail   \n",
       "\n",
       "                                                about  \\\n",
       "56  Mustika Ratu is a well-known Indonesian cosmet...   \n",
       "60  Holika-holika is a South Korean cosmetics bran...   \n",
       "80  Wardah is an Indonesian cosmetics and skincare...   \n",
       "83  The Body Shop is a global beauty and cosmetics...   \n",
       "27  Paragon Technology and Innovation is an Indone...   \n",
       "73  Nivea is a global brand that primarily focuses...   \n",
       "62  Damn I Love Indonesia (DILo) is a lifestyle br...   \n",
       "40  Head & Shoulders is a well-known brand of anti...   \n",
       "71  Bukalapak is an Indonesian online marketplace ...   \n",
       "42  Alfamart is a leading Indonesian convenience s...   \n",
       "\n",
       "                                             metadata  \n",
       "56  Sponsor Personal & Beauty Mustika Ratu is a we...  \n",
       "60  Sponsor Personal & Beauty Holika-holika is a S...  \n",
       "80  Sponsor Personal & Beauty Wardah is an Indones...  \n",
       "83  Sponsor Personal & Beauty The Body Shop is a g...  \n",
       "27  Sponsor Personal & Beauty Paragon Technology a...  \n",
       "73  Sponsor Personal & Beauty Nivea is a global br...  \n",
       "62  Sponsor Textiles & Apparel Damn I Love Indones...  \n",
       "40  Sponsor Personal & Beauty Head & Shoulders is ...  \n",
       "71  Sponsor E-Commerce Bukalapak is an Indonesian ...  \n",
       "42  Sponsor Distribution & Retail Alfamart is a le...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recsys.fit()\n",
    "recsys.recommend(66)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cab7416-1243-4d94-b273-d26f6e34689a",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5cb60b-0dd1-4ae8-9a0c-1cd6d02821a6",
   "metadata": {},
   "source": [
    "Model is already saved by the time we this the following code:\n",
    "`\n",
    "recsys.run()\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764df126-274b-440f-af01-cc35c2126756",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85a5789-257b-46bc-a627-e559ebe015cc",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1340a17e",
   "metadata": {},
   "source": [
    "#### 1. Import modules and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e678ecf2-9e2c-4d0e-9c34-272d14264f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Location</th>\n",
       "      <th>Price</th>\n",
       "      <th>Overall Rating</th>\n",
       "      <th>Number Sold</th>\n",
       "      <th>Total Review</th>\n",
       "      <th>Customer Rating</th>\n",
       "      <th>Customer Review</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Computers and Laptops</td>\n",
       "      <td>Wireless Keyboard i8 Mini TouchPad Mouse 2.4G ...</td>\n",
       "      <td>Jakarta Utara</td>\n",
       "      <td>53500</td>\n",
       "      <td>4.9</td>\n",
       "      <td>5449</td>\n",
       "      <td>2369</td>\n",
       "      <td>5</td>\n",
       "      <td>Alhamdulillah berfungsi dengan baik. Packaging...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Computers and Laptops</td>\n",
       "      <td>PAKET LISENSI WINDOWS 10 PRO DAN OFFICE 2019 O...</td>\n",
       "      <td>Kota Tangerang Selatan</td>\n",
       "      <td>72000</td>\n",
       "      <td>4.9</td>\n",
       "      <td>2359</td>\n",
       "      <td>1044</td>\n",
       "      <td>5</td>\n",
       "      <td>barang bagus dan respon cepat, harga bersaing ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Computers and Laptops</td>\n",
       "      <td>SSD Midasforce 128 Gb - Tanpa Caddy</td>\n",
       "      <td>Jakarta Barat</td>\n",
       "      <td>213000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12300</td>\n",
       "      <td>3573</td>\n",
       "      <td>5</td>\n",
       "      <td>barang bagus, berfungsi dengan baik, seler ram...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Computers and Laptops</td>\n",
       "      <td>ADAPTOR CHARGER MONITOR LCD LED TV LG merek LG...</td>\n",
       "      <td>Jakarta Timur</td>\n",
       "      <td>55000</td>\n",
       "      <td>4.7</td>\n",
       "      <td>2030</td>\n",
       "      <td>672</td>\n",
       "      <td>5</td>\n",
       "      <td>bagus sesuai harapan penjual nya juga ramah. t...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Computers and Laptops</td>\n",
       "      <td>ADAPTOR CHARGER MONITOR LCD LED TV LG merek LG...</td>\n",
       "      <td>Jakarta Timur</td>\n",
       "      <td>55000</td>\n",
       "      <td>4.7</td>\n",
       "      <td>2030</td>\n",
       "      <td>672</td>\n",
       "      <td>5</td>\n",
       "      <td>Barang Bagus, pengemasan Aman, dapat Berfungsi...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5395</th>\n",
       "      <td>Household</td>\n",
       "      <td>PCK-01 Penjepit Barang / Pickup Tool Ver.1 Ala...</td>\n",
       "      <td>Jakarta Pusat</td>\n",
       "      <td>35000</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1431</td>\n",
       "      <td>562</td>\n",
       "      <td>5</td>\n",
       "      <td>Harga bersaing, barang sesuai pesanan. Saya na...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5396</th>\n",
       "      <td>Household</td>\n",
       "      <td>Ultrasonic Aroma Diffuser Humidifier Colorful ...</td>\n",
       "      <td>Jakarta Utara</td>\n",
       "      <td>99000</td>\n",
       "      <td>4.9</td>\n",
       "      <td>15529</td>\n",
       "      <td>4074</td>\n",
       "      <td>5</td>\n",
       "      <td>Beli ini krn Anak &amp; Istri mau liburan di Jakar...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5397</th>\n",
       "      <td>Household</td>\n",
       "      <td>Ultrasonic Aroma Diffuser Humidifier Colorful ...</td>\n",
       "      <td>Jakarta Utara</td>\n",
       "      <td>99000</td>\n",
       "      <td>4.9</td>\n",
       "      <td>15529</td>\n",
       "      <td>4074</td>\n",
       "      <td>5</td>\n",
       "      <td>pengemasan barang baik, kondisi barang jg utuh...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5398</th>\n",
       "      <td>Household</td>\n",
       "      <td>Ultrasonic Aroma Diffuser Humidifier Colorful ...</td>\n",
       "      <td>Jakarta Utara</td>\n",
       "      <td>99000</td>\n",
       "      <td>4.9</td>\n",
       "      <td>15529</td>\n",
       "      <td>4074</td>\n",
       "      <td>5</td>\n",
       "      <td>Mungil tapi bekerja dng baik. Dan murahh terja...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5399</th>\n",
       "      <td>Household</td>\n",
       "      <td>TDS Meter 3 Alat Ukur Hidroponik Air Aquarium ...</td>\n",
       "      <td>Jakarta Utara</td>\n",
       "      <td>14400</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4765</td>\n",
       "      <td>3044</td>\n",
       "      <td>5</td>\n",
       "      <td>Produk sesuai deskripsi, packing aman terlindu...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Love</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5400 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Category  \\\n",
       "0     Computers and Laptops   \n",
       "1     Computers and Laptops   \n",
       "2     Computers and Laptops   \n",
       "3     Computers and Laptops   \n",
       "4     Computers and Laptops   \n",
       "...                     ...   \n",
       "5395              Household   \n",
       "5396              Household   \n",
       "5397              Household   \n",
       "5398              Household   \n",
       "5399              Household   \n",
       "\n",
       "                                           Product Name  \\\n",
       "0     Wireless Keyboard i8 Mini TouchPad Mouse 2.4G ...   \n",
       "1     PAKET LISENSI WINDOWS 10 PRO DAN OFFICE 2019 O...   \n",
       "2                   SSD Midasforce 128 Gb - Tanpa Caddy   \n",
       "3     ADAPTOR CHARGER MONITOR LCD LED TV LG merek LG...   \n",
       "4     ADAPTOR CHARGER MONITOR LCD LED TV LG merek LG...   \n",
       "...                                                 ...   \n",
       "5395  PCK-01 Penjepit Barang / Pickup Tool Ver.1 Ala...   \n",
       "5396  Ultrasonic Aroma Diffuser Humidifier Colorful ...   \n",
       "5397  Ultrasonic Aroma Diffuser Humidifier Colorful ...   \n",
       "5398  Ultrasonic Aroma Diffuser Humidifier Colorful ...   \n",
       "5399  TDS Meter 3 Alat Ukur Hidroponik Air Aquarium ...   \n",
       "\n",
       "                    Location   Price  Overall Rating  Number Sold  \\\n",
       "0              Jakarta Utara   53500             4.9         5449   \n",
       "1     Kota Tangerang Selatan   72000             4.9         2359   \n",
       "2              Jakarta Barat  213000             5.0        12300   \n",
       "3              Jakarta Timur   55000             4.7         2030   \n",
       "4              Jakarta Timur   55000             4.7         2030   \n",
       "...                      ...     ...             ...          ...   \n",
       "5395           Jakarta Pusat   35000             4.9         1431   \n",
       "5396           Jakarta Utara   99000             4.9        15529   \n",
       "5397           Jakarta Utara   99000             4.9        15529   \n",
       "5398           Jakarta Utara   99000             4.9        15529   \n",
       "5399           Jakarta Utara   14400             4.9         4765   \n",
       "\n",
       "      Total Review  Customer Rating  \\\n",
       "0             2369                5   \n",
       "1             1044                5   \n",
       "2             3573                5   \n",
       "3              672                5   \n",
       "4              672                5   \n",
       "...            ...              ...   \n",
       "5395           562                5   \n",
       "5396          4074                5   \n",
       "5397          4074                5   \n",
       "5398          4074                5   \n",
       "5399          3044                5   \n",
       "\n",
       "                                        Customer Review Sentiment Emotion  \n",
       "0     Alhamdulillah berfungsi dengan baik. Packaging...  Positive   Happy  \n",
       "1     barang bagus dan respon cepat, harga bersaing ...  Positive   Happy  \n",
       "2     barang bagus, berfungsi dengan baik, seler ram...  Positive   Happy  \n",
       "3     bagus sesuai harapan penjual nya juga ramah. t...  Positive   Happy  \n",
       "4     Barang Bagus, pengemasan Aman, dapat Berfungsi...  Positive   Happy  \n",
       "...                                                 ...       ...     ...  \n",
       "5395  Harga bersaing, barang sesuai pesanan. Saya na...  Positive    Love  \n",
       "5396  Beli ini krn Anak & Istri mau liburan di Jakar...  Positive    Love  \n",
       "5397  pengemasan barang baik, kondisi barang jg utuh...  Positive   Happy  \n",
       "5398  Mungil tapi bekerja dng baik. Dan murahh terja...  Positive   Happy  \n",
       "5399  Produk sesuai deskripsi, packing aman terlindu...  Positive    Love  \n",
       "\n",
       "[5400 rows x 11 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the dataset\n",
    "import pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout,Flatten, Bidirectional,GlobalMaxPooling1D,Conv1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Load the data\n",
    "file_path = 'PRDECT-ID Dataset.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da6ff7f",
   "metadata": {},
   "source": [
    "#### 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0e9e55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text data\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    return text\n",
    "\n",
    "# Clean the reviews\n",
    "data['Cleaned_Review'] = data['Customer Review'].apply(clean_text)\n",
    "\n",
    "# Initialize and fit the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['Cleaned_Review'])\n",
    "\n",
    "tokenizer_json = tokenizer.to_json()\n",
    "with open(\"nlp_tokenizer.json\", \"w\") as json_file:\n",
    "    json_file.write(tokenizer_json)\n",
    "\n",
    "# Convert text to sequences\n",
    "sequences = tokenizer.texts_to_sequences(data['Cleaned_Review'])\n",
    "\n",
    "# Pad the sequences to have uniform length\n",
    "max_length = 1000\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(data['Emotion'])\n",
    "joblib.dump(label_encoder, 'nlp_label_encoder.joblib')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, encoded_labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd9ecb9",
   "metadata": {},
   "source": [
    "#### 3. Data Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0940facc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "68/68 [==============================] - 31s 448ms/step - loss: 1.4179 - accuracy: 0.3877 - val_loss: 1.1184 - val_accuracy: 0.5176\n",
      "Epoch 2/10\n",
      "68/68 [==============================] - 30s 437ms/step - loss: 0.9132 - accuracy: 0.5961 - val_loss: 0.9089 - val_accuracy: 0.5944\n",
      "Epoch 3/10\n",
      "68/68 [==============================] - 33s 478ms/step - loss: 0.6013 - accuracy: 0.7711 - val_loss: 0.9275 - val_accuracy: 0.6000\n",
      "Epoch 4/10\n",
      "68/68 [==============================] - 32s 478ms/step - loss: 0.3308 - accuracy: 0.9000 - val_loss: 0.9991 - val_accuracy: 0.6157\n",
      "Epoch 5/10\n",
      "68/68 [==============================] - 34s 496ms/step - loss: 0.1495 - accuracy: 0.9657 - val_loss: 1.1655 - val_accuracy: 0.6176\n",
      "Epoch 6/10\n",
      "68/68 [==============================] - 29s 434ms/step - loss: 0.0879 - accuracy: 0.9780 - val_loss: 1.3228 - val_accuracy: 0.6083\n",
      "Epoch 7/10\n",
      "68/68 [==============================] - 31s 449ms/step - loss: 0.0617 - accuracy: 0.9850 - val_loss: 1.3918 - val_accuracy: 0.6028\n",
      "Epoch 8/10\n",
      "68/68 [==============================] - 30s 436ms/step - loss: 0.0469 - accuracy: 0.9896 - val_loss: 1.4832 - val_accuracy: 0.6046\n",
      "Epoch 9/10\n",
      "68/68 [==============================] - 30s 438ms/step - loss: 0.0439 - accuracy: 0.9880 - val_loss: 1.5095 - val_accuracy: 0.6065\n",
      "Epoch 10/10\n",
      "68/68 [==============================] - 33s 485ms/step - loss: 0.0403 - accuracy: 0.9905 - val_loss: 1.5588 - val_accuracy: 0.6028\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 1.5588 - accuracy: 0.6028\n",
      "Test Accuracy: 0.6027777791023254\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 128, input_length=max_length))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=3)]\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), callbacks=callbacks)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392f51b8",
   "metadata": {},
   "source": [
    "#### 4. Predicting emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "daea82a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 126ms/step\n",
      "Sentence: 'Produknya keren.' - Predicted Emotion: Love\n",
      "Sentence: 'Ga sesuai ekspektasi.' - Predicted Emotion: Sadness\n",
      "Sentence: 'Jangan beli di sini, penjual ga ramah.' - Predicted Emotion: Fear\n",
      "Sentence: 'Barangnya jelek.' - Predicted Emotion: Anger\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example Indonesian sentences\n",
    "sentences = [\"Produknya keren.\",\n",
    "             \"Ga sesuai ekspektasi.\",\n",
    "             \"Jangan beli di sini, penjual ga ramah.\",\n",
    "             \"Barangnya jelek.\"]\n",
    "\n",
    "# Preprocess the sentences (cleaning, tokenizing, and padding)\n",
    "cleaned_sentences = [clean_text(sentence) for sentence in sentences] \n",
    "encoded_sentences = tokenizer.texts_to_sequences(cleaned_sentences)\n",
    "padded_sentences = pad_sequences(encoded_sentences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Predict using the model\n",
    "predictions = model.predict(padded_sentences)\n",
    "predicted_emotions = [label_encoder.classes_[np.argmax(prediction)] for prediction in predictions]\n",
    "\n",
    "# Display predictions\n",
    "for sentence, emotion in zip(sentences, predicted_emotions):\n",
    "    print(f\"Sentence: '{sentence}' - Predicted Emotion: {emotion}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef27767",
   "metadata": {},
   "source": [
    "### Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "143ec1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "68/68 [==============================] - 30s 438ms/step - loss: 1.4337 - accuracy: 0.3935 - val_loss: 1.1597 - val_accuracy: 0.5157\n",
      "Epoch 2/10\n",
      "68/68 [==============================] - 30s 442ms/step - loss: 0.9540 - accuracy: 0.5748 - val_loss: 0.9345 - val_accuracy: 0.5889\n",
      "Epoch 3/10\n",
      "68/68 [==============================] - 29s 433ms/step - loss: 0.6592 - accuracy: 0.7544 - val_loss: 0.9062 - val_accuracy: 0.6176\n",
      "Epoch 4/10\n",
      "68/68 [==============================] - 29s 428ms/step - loss: 0.3686 - accuracy: 0.8947 - val_loss: 0.9945 - val_accuracy: 0.5981\n",
      "Epoch 5/10\n",
      "68/68 [==============================] - 28s 408ms/step - loss: 0.1758 - accuracy: 0.9546 - val_loss: 1.1322 - val_accuracy: 0.5889\n",
      "Epoch 6/10\n",
      "68/68 [==============================] - 28s 414ms/step - loss: 0.0922 - accuracy: 0.9755 - val_loss: 1.2590 - val_accuracy: 0.5769\n",
      "Epoch 7/10\n",
      "68/68 [==============================] - 28s 411ms/step - loss: 0.0618 - accuracy: 0.9838 - val_loss: 1.3143 - val_accuracy: 0.5824\n",
      "Epoch 8/10\n",
      "68/68 [==============================] - 27s 402ms/step - loss: 0.0464 - accuracy: 0.9891 - val_loss: 1.3935 - val_accuracy: 0.5769\n",
      "Epoch 9/10\n",
      "68/68 [==============================] - 29s 421ms/step - loss: 0.0392 - accuracy: 0.9898 - val_loss: 1.4718 - val_accuracy: 0.5731\n",
      "Epoch 10/10\n",
      "68/68 [==============================] - 27s 404ms/step - loss: 0.0388 - accuracy: 0.9903 - val_loss: 1.5077 - val_accuracy: 0.5731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\OneDrive - Institut Teknologi Bandung\\[AKADEMIK]\\Semester 7\\Bangkit\\.venv\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 2s 56ms/step - loss: 1.5077 - accuracy: 0.5731\n",
      "Test Loss:1.5076501369476318\n",
      "Test Accuracy: 0.5731481313705444\n",
      "<keras.src.engine.sequential.Sequential object at 0x00000235C250B8E0> <keras.src.callbacks.History object at 0x00000235C506A5B0>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import joblib\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "class SentimentAnalysis:\n",
    "    def __init__(self, \n",
    "                 data_path:str,\n",
    "                 sentences_col:str,\n",
    "                 label_col:str,\n",
    "                 max_length:int=1000,\n",
    "                 padding:str='post'):\n",
    "        \"\"\"\n",
    "            Initialize data and properties\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "            INPUT\n",
    "            data_path: where the data is located\n",
    "            sentences_col: column name in which the full sentences to be trained\n",
    "            label_col: column name in which the label of the corresponding sentence to be trained\n",
    "            max_length: number maximum length of a sequence \n",
    "            padding: technique to ensure that all sequences in a dataset have the same length\n",
    "\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(data_path)\n",
    "        self.sentences_col = self.data[sentences_col]\n",
    "        self.label_col = self.data[label_col]\n",
    "        self.max_length = max_length\n",
    "        self.padding = padding\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"\n",
    "            To perform several text cleaning operations\n",
    "        \"\"\"\n",
    "        \"\"\" \n",
    "            INPUT\n",
    "            text: the input sentence to be cleaned\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "            OUTPUT\n",
    "            preprocessed text with lowercase characters, non-alphanumeric characters, non-whitespace characters,\n",
    "            and non-numeric characters.\n",
    "        \"\"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        return text\n",
    "\n",
    "    def initialize_tokenizer(self,\n",
    "                             save_tokenizer:str=None):\n",
    "        \"\"\"\n",
    "            Fit the tokenizer for the text and save it\n",
    "        \"\"\"\n",
    "        \"\"\" \n",
    "            INPUT\n",
    "            save_tokenizer: name of the file in which the tokenizer used in this model. \n",
    "            If not None, the tokenizer will be saved in json format.\n",
    "        \"\"\"\n",
    "        self.data['cleaned_col'] = self.sentences_col.apply(self.clean_text)\n",
    "        self.tokenizer = Tokenizer()\n",
    "        self.tokenizer.fit_on_texts(self.data['cleaned_col'])\n",
    "        self.vocab_size = len(self.tokenizer.word_index) + 1\n",
    "\n",
    "        # Save tokenizer to JSON\n",
    "        if save_tokenizer is not None:\n",
    "            tokenizer_json = self.tokenizer.to_json()\n",
    "            with open(f\"{save_tokenizer}\", \"w\") as json_file:\n",
    "                json_file.write(tokenizer_json)        \n",
    "\n",
    "    def encode_labels(self, \n",
    "                      save_encoder:str=None):\n",
    "        \"\"\"\n",
    "            Fit the label encoder for the label and save it\n",
    "        \"\"\"\n",
    "        \"\"\" \n",
    "            INPUT\n",
    "            save_encoder: name of the file in which the label encoder used in this model. \n",
    "            If not None, the label encoder will be saved in joblib format.\n",
    "        \"\"\"\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.label_encoder.fit(self.label_col)\n",
    "        if save_encoder is not None:\n",
    "            joblib.dump(self.label_encoder, f'{save_encoder}')\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        \"\"\"\n",
    "            Using tokenizer and label encoder, perform the data preprocessing and data splotting\n",
    "        \"\"\"\n",
    "        self.sequences = self.tokenizer.texts_to_sequences(self.data['cleaned_col'])\n",
    "        self.padded_sequences = pad_sequences(self.sequences, maxlen=self.max_length, padding=self.padding)\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            self.padded_sequences, \n",
    "            self.label_encoder.transform(self.data['Emotion']),\n",
    "            test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "    def create_model(self):\n",
    "        \"\"\"\n",
    "            Create the model architecture and compiler using Tensorflow\n",
    "        \"\"\"\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Embedding(self.vocab_size, 128, input_length=self.max_length))\n",
    "        self.model.add(Conv1D(128, 5, activation='relu'))\n",
    "        self.model.add(GlobalMaxPooling1D())\n",
    "        self.model.add(Dense(64, activation='relu'))\n",
    "        self.model.add(Dense(len(self.label_encoder.classes_), activation='softmax'))\n",
    "\n",
    "        # Smaller learning rate\n",
    "        optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "        self.model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    def train_model(self,\n",
    "                    save_model:str=\"nlp_emotion_model.h5\",\n",
    "                    save_tokenizer:str='nlp_tokenizer.json',\n",
    "                    save_encoder:str='nlp_label_encoder.joblib'):\n",
    "        \"\"\"\n",
    "            Using Tensorflow, train the data to establish the model\n",
    "        \"\"\"\n",
    "        \"\"\" \n",
    "            INPUT\n",
    "            save_model: path where the model will be saved (None if not saving the model)\n",
    "            save_tokenizer: path where the tokenizer will be saved (None if not saving the model)\n",
    "            save_encoder: path where the label encoder will be saved (None if not saving the encoder)\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "            OUTPUT\n",
    "            model and training history\n",
    "        \"\"\"\n",
    "        self.initialize_tokenizer(save_tokenizer=save_tokenizer)\n",
    "        self.encode_labels(save_encoder=save_encoder)\n",
    "        self.preprocess_data()\n",
    "        self.create_model()\n",
    "        callbacks = [tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=3)]\n",
    "\n",
    "        history = self.model.fit(self.X_train, self.y_train, epochs=10, batch_size=64, validation_data=(self.X_test, self.y_test), callbacks=callbacks)\n",
    "        if save_model is not None:\n",
    "            self.model.save(f\"{save_model}\")\n",
    "\n",
    "        return self.model, history\n",
    "\n",
    "    def evaluate_model(self):\n",
    "        \"\"\"\n",
    "            Evaluate the model\n",
    "        \"\"\"\n",
    "        loss, accuracy = self.model.evaluate(self.X_test, self.y_test)\n",
    "        print(f'Test Loss:{loss}\\nTest Accuracy: {accuracy}')\n",
    "\n",
    "\n",
    "__name__ = \"__main__\"\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = 'PRDECT-ID Dataset.csv'\n",
    "\n",
    "    SA = SentimentAnalysis(data_path=file_path,\n",
    "                           sentences_col='Customer Review',\n",
    "                           label_col='Emotion',)\n",
    "    model,history = SA.train_model()\n",
    "    SA.evaluate_model()\n",
    "    print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26fdcdb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 81ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Anger': 0.3838447630405426,\n",
       " 'Fear': 0.029447946697473526,\n",
       " 'Happy': 0.004473392385989428,\n",
       " 'Love': 0.2464277744293213,\n",
       " 'Sadness': 0.3358061611652374}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer,tokenizer_from_json\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "import joblib\n",
    "\n",
    "import psycopg2 as pg2\n",
    "from psycopg2.extras import RealDictCursor\n",
    "import time\n",
    "import pandas as pd\n",
    "class NLP_emotion:\n",
    "    def __init__(self,\n",
    "                 model_path,\n",
    "                 label_encoder_path,\n",
    "                 tokenizer_path, \n",
    "                 set_of_sentences:np.ndarray):\n",
    "        \"\"\"\n",
    "            Initialize data, label encoder, and tokenizer.\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "            INPUT\n",
    "            model_path: model's path\n",
    "            label_encoder_path: label encoder's path \n",
    "            tokenizer_path: tokenizer's path \n",
    "            set_of_sentences: sentences to be predicted \n",
    "        \"\"\"\n",
    "        model =  tf.keras.models.load_model(model_path)\n",
    "        label_encoder = joblib.load(label_encoder_path)\n",
    "        with open(tokenizer_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "            loaded_tokenizer_json = json_file.read()\n",
    "        tokenizer = tokenizer_from_json(loaded_tokenizer_json)\n",
    "        self.model=model\n",
    "        self.label_encoder=label_encoder\n",
    "        self.tokenizer=tokenizer\n",
    "        self.sentences=set_of_sentences\n",
    "\n",
    "    def clean_text(self,text):\n",
    "        \"\"\"\n",
    "            To perform several text cleaning operations.\n",
    "        \"\"\"\n",
    "        \"\"\" \n",
    "            INPUT\n",
    "            text: the input sentence to be cleaned\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "            OUTPUT\n",
    "            preprocessed text with lowercase characters, non-alphanumeric characters, non-whitespace characters,\n",
    "            and non-numeric characters.\n",
    "        \"\"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        return text\n",
    "\n",
    "    def predict_emotions(self):\n",
    "        \"\"\"\n",
    "            To predict the emotions of the sentences using preloaded model, tokenizer, and label encoder.\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "            OUTPUT\n",
    "            Dictionary of emotion (\"Anger\",\"Fear\",\"Happy\",\"Love\",\"Sadness\") and its percentage per sentence.\n",
    "        \"\"\"\n",
    "        max_length = 1000\n",
    "        cleaned_sentences = [self.clean_text(sentence) for sentence in self.sentences]  \n",
    "        encoded_sentences = self.tokenizer.texts_to_sequences(cleaned_sentences)\n",
    "        padded_sentences = pad_sequences(encoded_sentences, maxlen=max_length, padding='post')\n",
    "        predictions = self.model.predict(padded_sentences)\n",
    "        return predictions\n",
    "    \n",
    "    def percentage_emotions(self):\n",
    "        \"\"\"\n",
    "            To create a comparation ratio of the emotions for all sentences collectively.\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "            OUTPUT\n",
    "            Dictionary of portion for all sentences collectively.\n",
    "        \"\"\"\n",
    "        predictions = self.predict_emotions()\n",
    "        emotion_sums = np.sum(predictions, axis=0)\n",
    "        emotion_averages = emotion_sums / len(predictions)\n",
    "        portion_emotion = {label: float(portion) for label, portion in zip(self.label_encoder.classes_, emotion_averages)}\n",
    "        return portion_emotion\n",
    "    \n",
    "sentences = [\"Produknya keren.\",\n",
    "             \"Ga sesuai ekspektasi.\",\n",
    "             \"Jangan beli di sini, penjual ga ramah.\",\n",
    "             \"Barangnya jelek.\"]\n",
    "\n",
    "nlp_class_coll = NLP_emotion(model_path='nlp_emotion_model.h5',\n",
    "                        label_encoder_path='nlp_label_encoder.joblib',\n",
    "                        tokenizer_path='nlp_tokenizer.json',\n",
    "                        set_of_sentences=sentences)\n",
    "nlp_class_coll.percentage_emotions()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
